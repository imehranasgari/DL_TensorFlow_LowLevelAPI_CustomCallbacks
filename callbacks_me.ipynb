{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6e1205-cbfa-4411-accd-ba3f7c530fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13596), started 0:30:53 ago. (Use '!kill 13596' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c8ea8899b19d3f0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c8ea8899b19d3f0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.19.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler, ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "## یعنی می خوام همینجا بیارش بالا\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5970f531-2180-4291-b70b-78822030f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds , info = tfds.load('horses_or_humans', as_supervised=True, with_info=True,split=['train[:80%]','train[80%:]','test'])\n",
    "\n",
    "(train_ds, validation_ds, test_ds) = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e7b233-06ae-45ad-a891-fe49ccba4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = info.splits['train'].num_examples\n",
    "num_classes = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d6df14-c50e-4cc0-a932-72cafc103454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(num_examples)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165438ce-8b3d-4f38-9d5c-fa9173968e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE =(150,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f96e88-7b08-40b5-886c-ce2ef1849c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocecing (image , label):\n",
    "    return tf.divide(tf.cast(tf.image.resize(image,IMAGE_SIZE),tf.float32),255.0) ,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5341ff7-6af4-4b89-903a-c8e7a8bba888",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a13d0f-ca0a-4e75-9ae6-63ccc99bcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_ds.shuffle(num_examples//4).map(preprocecing).batch(BATCH_SIZE).prefetch(1)\n",
    "validation_batches = validation_ds.map(preprocecing).batch(BATCH_SIZE).prefetch(1)\n",
    "test_batches = test_ds.map(preprocecing).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7887048b-af5b-4cb3-83cb-8aa789c69128",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch , label_batch = next(iter(train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc1c02e1-dbc1-4e5a-9b30-986b721249db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 150, 150, 3) & (32,)\n"
     ]
    }
   ],
   "source": [
    "print(image_batch.shape ,'&', label_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db590673-63db-4233-855b-76c1fa051161",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The expression `input_shape=IMAGE_SIZE + (3,)` in Python is a combination of **tuples**, used to specify the **input dimensions of images** for the first layer of a neural network (here, `Conv2D`). Let’s break it down step by step:\n",
    "\n",
    "***\n",
    "\n",
    "### 1. The value of `IMAGE_SIZE`\n",
    "\n",
    "```python\n",
    "IMAGE_SIZE = (150, 150)\n",
    "```\n",
    "\n",
    "This means the input images should be 150 pixels wide and 150 pixels tall.\n",
    "\n",
    "***\n",
    "\n",
    "### 2. The expression `(3,)`\n",
    "\n",
    "This is a **single-element tuple** with the value 3. The number 3 represents the **number of color channels**:\n",
    "\n",
    "- For a color image: `3` (RGB)\n",
    "- For a grayscale image: `1`\n",
    "\n",
    "***\n",
    "\n",
    "### 3. Adding Tuples\n",
    "\n",
    "```python\n",
    "IMAGE_SIZE + (3,)\n",
    "```\n",
    "\n",
    "In Python, when you add two tuples with the `+` operator, they are **concatenated**. The result:\n",
    "\n",
    "```python\n",
    "(150, 150) + (3,) → (150, 150, 3)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### 4. Final Result: `input_shape=(150, 150, 3)`\n",
    "\n",
    "This means that each input image to the network should have the shape:\n",
    "\n",
    "- **150 pixels height**\n",
    "- **150 pixels width**\n",
    "- **3 color channels (RGB)**\n",
    "\n",
    "This value is passed as a parameter to `Conv2D` so it knows what shape of data to expect.\n",
    "\n",
    "***\n",
    "\n",
    "### Summary\n",
    "\n",
    "```python\n",
    "input_shape=IMAGE_SIZE + (3,)\n",
    "```\n",
    "\n",
    "is a Pythonic way to write `input_shape=(150, 150, 3)` dynamically and flexibly. For example, if you change the image size later, you won't have to manually alter `input_shape`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "213fac37-25dd-422d-b4f0-fca1f953b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dense_units, input_shape=IMAGE_SIZE + (3,)):\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Input(shape=input_shape),  # ✅ لایه Input به صورت جداگانه\n",
    "      tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2, 2),\n",
    "      tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2, 2),\n",
    "      tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2, 2),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(dense_units, activation='relu'),\n",
    "      tf.keras.layers.Dense(2, activation='softmax')\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81aad4-9e7d-4ecb-a013-725fe7a1f03f",
   "metadata": {},
   "source": [
    "## TensorBoard Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bd702-e248-4e8c-9d21-ad5764a4fc03",
   "metadata": {},
   "source": [
    "First, we remove the folder if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db61d50-77eb-4d55-a8c4-d58f971922b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. اگر موجودیتی به نام \"logs\" هست (فایل یا پوشه)، آن را حذف کن\n",
    "if os.path.exists(\"logs\"):\n",
    "    if os.path.isdir(\"logs\"):\n",
    "        shutil.rmtree(\"logs\")\n",
    "    else:\n",
    "        os.remove(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0512f9d4-22c6-46f4-b68c-d77a9d4dfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. یک پوشه‌ی خالی به نام \"logs\" بساز\n",
    "os.makedirs(\"logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78ea927a-d935-4ee1-a24c-4da393c2b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. مسیر لاگ جدید با Timestamp\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# (اختیاری) مطمئن شو پوشه‌ی زیرمجموعه هم ساخته می‌شود\n",
    "os.makedirs(logdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cc5cff9-9b69-4252-acca-9b85c7fa7606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['.anaconda', '.cache', '.conda', '.condarc', '.continuum', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.matplotlib', '.virtual_documents', '.vscode', 'anaconda_projects', 'ansel', 'AppData', 'Application Data', 'callbacks_me.ipynb', 'Contacts', 'Cookies', 'Desktop', 'Documents', 'Downloads', 'Favorites', 'Intel', 'Links', 'Local Settings', 'logs', 'Music', 'My Documents', 'NetHood', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{a2332f18-cdbf-11ec-8680-002248483d79}.TM.blf', 'NTUSER.DAT{a2332f18-cdbf-11ec-8680-002248483d79}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{a2332f18-cdbf-11ec-8680-002248483d79}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'Pictures', 'PrintHood', 'Recent', 'Saved Games', 'Searches', 'SendTo', 'Start Menu', 'Templates', 'tensorflow_datasets', 'Videos']\n",
      "After: ['20250518-151721']\n"
     ]
    }
   ],
   "source": [
    "# قبل از حذف\n",
    "print(\"Before:\", os.listdir(\".\"))      # لیست فایل و پوشه‌های جاری\n",
    "# (پس از حذف/ساخت)\n",
    "print(\"After:\", os.listdir(\"logs\"))     # باید خالی باشد یا فقط timestamp زیرمجموعه\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfd5b3f0-ee2f-449b-a905-d7e62fc882f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. تعریف و کامپایل مدل\n",
    "model = build_model(dense_units=256)\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa5ae533-234e-424e-a33a-b1d4f2cdf3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ساخت Callback با نام آرگومان صحیح\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, \n",
    "                                                      histogram_freq=1,\n",
    "                                                      update_freq=1,\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04e5f2f5-76b6-457e-9323-36563ed3c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.5045 - loss: 0.6910 - val_accuracy: 0.5171 - val_loss: 0.6779\n",
      "Epoch 2/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.5854 - loss: 0.6625 - val_accuracy: 0.7366 - val_loss: 0.6314\n",
      "Epoch 3/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.6587 - loss: 0.6242 - val_accuracy: 0.7268 - val_loss: 0.5801\n",
      "Epoch 4/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.7371 - loss: 0.5622 - val_accuracy: 0.7268 - val_loss: 0.5314\n",
      "Epoch 5/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.7079 - loss: 0.5637 - val_accuracy: 0.7659 - val_loss: 0.4934\n",
      "Epoch 6/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.7719 - loss: 0.4882 - val_accuracy: 0.7659 - val_loss: 0.4552\n",
      "Epoch 7/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8189 - loss: 0.4351 - val_accuracy: 0.8732 - val_loss: 0.3565\n",
      "Epoch 8/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.8511 - loss: 0.3571 - val_accuracy: 0.9024 - val_loss: 0.3017\n",
      "Epoch 9/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.8866 - loss: 0.3194 - val_accuracy: 0.8878 - val_loss: 0.3123\n",
      "Epoch 10/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9153 - loss: 0.2446 - val_accuracy: 0.9610 - val_loss: 0.2063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15663306860>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. آموزش\n",
    "model.fit(train_batches,\n",
    "          epochs=10,\n",
    "          validation_data=validation_batches,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a90f071-94d0-4e40-9e45-3b245d8fcc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-64fd78c97df77709\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-64fd78c97df77709\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs --port 6007 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccc42f-7508-41e7-a93c-91667c77127d",
   "metadata": {},
   "source": [
    "## Saving Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39e628cc-22c6-4795-82e6-2fb447992090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# ساخت پوشه checkpoints اگر وجود نداشته باشد\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# تنظیم ModelCheckpoint برای ذخیره در پوشه checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # در اینجا فقط ویت\n",
    "    filepath=os.path.join(checkpoint_dir, 'weights.{epoch:02d}-{val_loss:.2f}.keras'),\n",
    "    verbose=1,\n",
    "    save_best_only=False,  # یا True اگر فقط بهترین مدل را می‌خواهید\n",
    "    save_weights_only=False,  # برای ذخیره کل مدل\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d9b86798-6f32-4cfe-82cd-fc0fb236d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\n",
      "Epoch 1: saving model to checkpoints\\weights.01-0.66.keras\n",
      "26/26 - 1s - 51ms/step - accuracy: 0.6204 - loss: 0.6737 - val_accuracy: 0.6244 - val_loss: 0.6618\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 2: saving model to checkpoints\\weights.02-0.61.keras\n",
      "26/26 - 1s - 37ms/step - accuracy: 0.6898 - loss: 0.6326 - val_accuracy: 0.6537 - val_loss: 0.6105\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 3: saving model to checkpoints\\weights.03-0.56.keras\n",
      "26/26 - 1s - 38ms/step - accuracy: 0.6861 - loss: 0.5912 - val_accuracy: 0.7220 - val_loss: 0.5621\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 4: saving model to checkpoints\\weights.04-0.54.keras\n",
      "26/26 - 1s - 36ms/step - accuracy: 0.7251 - loss: 0.5548 - val_accuracy: 0.7561 - val_loss: 0.5425\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 5: saving model to checkpoints\\weights.05-0.63.keras\n",
      "26/26 - 1s - 38ms/step - accuracy: 0.8017 - loss: 0.4784 - val_accuracy: 0.6049 - val_loss: 0.6321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15644465bd0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(dense_units=256)\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "  \n",
    "model.fit(\n",
    "    train_batches,\n",
    "    epochs=5,\n",
    "    validation_data=validation_batches,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2620c4-1449-4b93-90eb-b031e80e12e3",
   "metadata": {},
   "source": [
    "## EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92e12e51-71c0-403f-b14b-8445fcbb718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=3,\n",
    "              min_delta=0.05,\n",
    "              baseline=0.8,\n",
    "              mode='min',\n",
    "              monitor='val_loss',\n",
    "              restore_best_weights=True,\n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f076ee85-d546-4f19-96d6-77e014ae884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 - 1s - 47ms/step - accuracy: 0.6058 - loss: 0.6705 - val_accuracy: 0.5756 - val_loss: 0.6592\n",
      "Epoch 2/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.6983 - loss: 0.6245 - val_accuracy: 0.6098 - val_loss: 0.6158\n",
      "Epoch 3/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.7129 - loss: 0.5883 - val_accuracy: 0.5610 - val_loss: 0.6325\n",
      "Epoch 4/50\n",
      "26/26 - 1s - 38ms/step - accuracy: 0.7470 - loss: 0.5450 - val_accuracy: 0.7122 - val_loss: 0.5461\n",
      "Epoch 5/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.7579 - loss: 0.5075 - val_accuracy: 0.8927 - val_loss: 0.4358\n",
      "Epoch 6/50\n",
      "26/26 - 1s - 36ms/step - accuracy: 0.8273 - loss: 0.4301 - val_accuracy: 0.8683 - val_loss: 0.3832\n",
      "Epoch 7/50\n",
      "26/26 - 1s - 36ms/step - accuracy: 0.8382 - loss: 0.3919 - val_accuracy: 0.8780 - val_loss: 0.3366\n",
      "Epoch 8/50\n",
      "26/26 - 1s - 37ms/step - accuracy: 0.8771 - loss: 0.3353 - val_accuracy: 0.9317 - val_loss: 0.2553\n",
      "Epoch 9/50\n",
      "26/26 - 1s - 36ms/step - accuracy: 0.9221 - loss: 0.2570 - val_accuracy: 0.7220 - val_loss: 0.4351\n",
      "Epoch 10/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.9246 - loss: 0.2341 - val_accuracy: 0.9463 - val_loss: 0.1648\n",
      "Epoch 11/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.9440 - loss: 0.1817 - val_accuracy: 0.9512 - val_loss: 0.1476\n",
      "Epoch 12/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.9659 - loss: 0.1317 - val_accuracy: 0.9854 - val_loss: 0.0950\n",
      "Epoch 13/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.9757 - loss: 0.1053 - val_accuracy: 0.9854 - val_loss: 0.0821\n",
      "Epoch 14/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.9550 - loss: 0.1564 - val_accuracy: 0.9854 - val_loss: 0.0863\n",
      "Epoch 15/50\n",
      "26/26 - 1s - 35ms/step - accuracy: 0.9842 - loss: 0.0831 - val_accuracy: 0.9854 - val_loss: 0.0677\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15646949db0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(dense_units=256)\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "  \n",
    "model.fit(train_batches, \n",
    "          epochs=50, \n",
    "          validation_data=validation_batches, \n",
    "          verbose=2,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd5d20-05c9-4a32-990b-fa6112103450",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "04b54ea6-fbb3-457b-9e3c-f65370deaa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(dense_units=256)\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc568b0-b442-4042-a200-67f93c1e504e",
   "metadata": {},
   "source": [
    "This line of code reduces the learning rate (`lr`) based on a formula:\n",
    "- `initial_lr`: The initial learning rate.\n",
    "- `drop`: The reduction factor (e.g., 0.5 for halving).\n",
    "- `epoch`: The current epoch number.\n",
    "- `epochs_drop`: The number of epochs after which the learning rate is reduced.\n",
    "\n",
    "Formula:\n",
    "- Every `epochs_drop` epochs, the learning rate is multiplied by `drop` (in a power-based manner).\n",
    "- `math.floor((1+epoch)/epochs_drop)` determines how many times the reduction should be applied.\n",
    "\n",
    "Example: If `initial_lr=0.1`, `drop=0.5`, `epochs_drop=10`:\n",
    "- Up to epoch 9: `lr = 0.1`\n",
    "- Epochs 10–19: `lr = 0.05`\n",
    "- Epochs 20–29: `lr = 0.025`\n",
    "\n",
    "Summary: The learning rate is decreased stepwise at specified intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bf7f795b-00aa-4d13-9c0c-8d5de8252819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "\tinitial_lr = 0.01\n",
    "\tdrop = 0.5  \n",
    "\tepochs_drop = 1\n",
    "\tlr = initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "\treturn lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1430fe-0b62-4b8c-9a67-62bd0dad4c93",
   "metadata": {},
   "source": [
    "In Python, `math.pow` and `math.floor` are functions from the `math` module used for mathematical calculations. Brief explanation:\n",
    "\n",
    "- **`math.pow(x, y)`**:\n",
    "  - **Function**: Calculates `x` raised to the power of `y` (`x^y`).\n",
    "  - **Output**: Always returns a floating-point number (float).\n",
    "  - **Example**:\n",
    "    ```python\n",
    "    import math\n",
    "    print(math.pow(2, 3))  # Output: 8.0\n",
    "    print(math.pow(5, 2))  # Output: 25.0\n",
    "    ```\n",
    "\n",
    "- **`math.floor(x)`**:\n",
    "  - **Function**: Returns the largest integer less than or equal to `x` (rounds down).\n",
    "  - **Output**: Returns an integer (`int`).\n",
    "  - **Example**:\n",
    "    ```python\n",
    "    import math\n",
    "    print(math.floor(3.7))   # Output: 3\n",
    "    print(math.floor(3.2))   # Output: 3\n",
    "    print(math.floor(-3.2))  # Output: -4\n",
    "    ```\n",
    "\n",
    "### Notes:\n",
    "- To use these, you must import the `math` module with `import math`.\n",
    "- `math.pow` is similar to the `**` operator, but always returns a float.\n",
    "- `math.floor` is used to remove the fractional part and get the lower integer.\n",
    "\n",
    "These functions are commonly used in calculations such as setting the learning rate (like in the previous code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ace13c1e-af3a-4cc5-96b5-892199123109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 1/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.4868 - loss: 0.6932 - val_accuracy: 0.6146 - val_loss: 0.6823 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0025.\n",
      "Epoch 2/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.6427 - loss: 0.6742 - val_accuracy: 0.5268 - val_loss: 0.6763 - learning_rate: 0.0025\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.00125.\n",
      "Epoch 3/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6288 - loss: 0.6640 - val_accuracy: 0.5854 - val_loss: 0.6721 - learning_rate: 0.0012\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.000625.\n",
      "Epoch 4/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6429 - loss: 0.6610 - val_accuracy: 0.5805 - val_loss: 0.6705 - learning_rate: 6.2500e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0003125.\n",
      "Epoch 5/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.6349 - loss: 0.6592 - val_accuracy: 0.5854 - val_loss: 0.6696 - learning_rate: 3.1250e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15645931c30>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_batches, \n",
    "          epochs=5, \n",
    "          validation_data=validation_batches, \n",
    "          callbacks=[LearningRateScheduler(step_decay, verbose=1),\n",
    "                    TensorBoard(log_dir=logdir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4556523-2921-4b52-9b48-0e52adb2f48e",
   "metadata": {},
   "source": [
    "This line of code adjusts the learning rate (`lr`) using exponential decay. Summary:\n",
    "\n",
    "- **`lr`**: The current learning rate (a numeric value).\n",
    "- **`tf.math.exp(-0.1)`**: Computes the exponential of `-0.1` (i.e., $$ e^{-0.1} \\approx 0.9048 $$).\n",
    "- **Function**: The learning rate (`lr`) is multiplied by the constant $$ e^{-0.1} $$ (approximately 0.9048).\n",
    "- **Result**: The learning rate decreases exponentially (about 9.52% decrease with each execution).\n",
    "\n",
    "### Example:\n",
    "\n",
    "If `lr = 0.1`:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "lr = 0.1\n",
    "new_lr = lr * tf.math.exp(-0.1)\n",
    "print(new_lr)  # Output: ~0.09048\n",
    "```\n",
    "\n",
    "### Usage:\n",
    "\n",
    "- This formula is commonly used in machine learning model optimization for gradual learning rate reduction, helping the model converge more precisely over time.\n",
    "- The value `-0.1` controls the decay rate (more negative values result in faster decay).\n",
    "\n",
    "Summary: It decreases the learning rate by a fixed exponential coefficient ($$ e^{-0.1} $$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0cd7b069-50c3-421e-8f8d-b807c82b946a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.5431 - loss: 0.6823 - val_accuracy: 0.4341 - val_loss: 0.7528 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.6183 - loss: 0.6425 - val_accuracy: 0.6634 - val_loss: 0.6162 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.7590 - loss: 0.5668 - val_accuracy: 0.6927 - val_loss: 0.5782 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 4/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.7426 - loss: 0.5115 - val_accuracy: 0.8244 - val_loss: 0.4577 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 5/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.8240 - loss: 0.4288 - val_accuracy: 0.8634 - val_loss: 0.3744 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 6/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.8365 - loss: 0.3971 - val_accuracy: 0.8976 - val_loss: 0.3230 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 7/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.8727 - loss: 0.3304 - val_accuracy: 0.8976 - val_loss: 0.2752 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 8/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9069 - loss: 0.2770 - val_accuracy: 0.9561 - val_loss: 0.2092 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 9/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9472 - loss: 0.2092 - val_accuracy: 0.9512 - val_loss: 0.1798 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 10/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9346 - loss: 0.1850 - val_accuracy: 0.9805 - val_loss: 0.1249 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.009048374092987754.\n",
      "Epoch 11/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.9771 - loss: 0.1231 - val_accuracy: 0.9659 - val_loss: 0.1173 - learning_rate: 0.0090\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.008187307806082011.\n",
      "Epoch 12/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9688 - loss: 0.1046 - val_accuracy: 0.9463 - val_loss: 0.1387 - learning_rate: 0.0082\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.007408182703889377.\n",
      "Epoch 13/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9869 - loss: 0.0813 - val_accuracy: 0.9805 - val_loss: 0.0792 - learning_rate: 0.0074\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.006703200903374146.\n",
      "Epoch 14/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9851 - loss: 0.0803 - val_accuracy: 0.9756 - val_loss: 0.0899 - learning_rate: 0.0067\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.006065306936257675.\n",
      "Epoch 15/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.9832 - loss: 0.0770 - val_accuracy: 0.9707 - val_loss: 0.0731 - learning_rate: 0.0061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15649b727d0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(dense_units=256)\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "  \n",
    "def exp_decay_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1).numpy()\n",
    "\n",
    "model.fit(train_batches, \n",
    "          epochs=15, \n",
    "          validation_data=validation_batches, \n",
    "          callbacks=[LearningRateScheduler(exp_decay_scheduler, verbose=1),\n",
    "                    TensorBoard(log_dir='./log_dir')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ca6d3-e719-4252-86b5-0efa5a5f2434",
   "metadata": {},
   "source": [
    "## custom callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ffa8db-5b57-4fef-a2a0-125cdf991817",
   "metadata": {},
   "source": [
    "How all types of custom callback formats function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e76f992b-dd5f-45fb-a33d-74d6e44a9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self,logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "    def on_train_end(self,logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(f'stoping training : got log keys: {keys}')\n",
    "    def on_batch_begin (self,epoch, logs =None):\n",
    "        keys = list(logs.keys())\n",
    "        print(f\"Start epoch {epoch} of training; got log keys: {keys}\")     \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e8a498a9-56f4-4109-a6d1-18ef28c00e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training; got log keys: []\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 7; got log keys: []\n",
      "...Training: end of batch 7; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 8; got log keys: []\n",
      "...Training: end of batch 8; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 9; got log keys: []\n",
      "...Training: end of batch 9; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 10; got log keys: []\n",
      "...Training: end of batch 10; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 11; got log keys: []\n",
      "...Training: end of batch 11; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 12; got log keys: []\n",
      "...Training: end of batch 12; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 13; got log keys: []\n",
      "...Training: end of batch 13; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 14; got log keys: []\n",
      "...Training: end of batch 14; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 15; got log keys: []\n",
      "...Training: end of batch 15; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 16; got log keys: []\n",
      "...Training: end of batch 16; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 17; got log keys: []\n",
      "...Training: end of batch 17; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 18; got log keys: []\n",
      "...Training: end of batch 18; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 19; got log keys: []\n",
      "...Training: end of batch 19; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 20; got log keys: []\n",
      "...Training: end of batch 20; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 21; got log keys: []\n",
      "...Training: end of batch 21; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 22; got log keys: []\n",
      "...Training: end of batch 22; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 23; got log keys: []\n",
      "...Training: end of batch 23; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 24; got log keys: []\n",
      "...Training: end of batch 24; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 25; got log keys: []\n",
      "...Training: end of batch 25; got log keys: ['accuracy', 'loss']\n",
      "Start testing; got log keys: []\n",
      "...Evaluating: start of batch 0; got log keys: []\n",
      "...Evaluating: end of batch 0; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 1; got log keys: []\n",
      "...Evaluating: end of batch 1; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 2; got log keys: []\n",
      "...Evaluating: end of batch 2; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 3; got log keys: []\n",
      "...Evaluating: end of batch 3; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 4; got log keys: []\n",
      "...Evaluating: end of batch 4; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 5; got log keys: []\n",
      "...Evaluating: end of batch 5; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 6; got log keys: []\n",
      "...Evaluating: end of batch 6; got log keys: ['accuracy', 'loss']\n",
      "Stop testing; got log keys: ['accuracy', 'loss']\n",
      "End epoch 0 of training; got log keys: ['accuracy', 'loss', 'val_accuracy', 'val_loss']\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 7; got log keys: []\n",
      "...Training: end of batch 7; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 8; got log keys: []\n",
      "...Training: end of batch 8; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 9; got log keys: []\n",
      "...Training: end of batch 9; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 10; got log keys: []\n",
      "...Training: end of batch 10; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 11; got log keys: []\n",
      "...Training: end of batch 11; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 12; got log keys: []\n",
      "...Training: end of batch 12; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 13; got log keys: []\n",
      "...Training: end of batch 13; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 14; got log keys: []\n",
      "...Training: end of batch 14; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 15; got log keys: []\n",
      "...Training: end of batch 15; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 16; got log keys: []\n",
      "...Training: end of batch 16; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 17; got log keys: []\n",
      "...Training: end of batch 17; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 18; got log keys: []\n",
      "...Training: end of batch 18; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 19; got log keys: []\n",
      "...Training: end of batch 19; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 20; got log keys: []\n",
      "...Training: end of batch 20; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 21; got log keys: []\n",
      "...Training: end of batch 21; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 22; got log keys: []\n",
      "...Training: end of batch 22; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 23; got log keys: []\n",
      "...Training: end of batch 23; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 24; got log keys: []\n",
      "...Training: end of batch 24; got log keys: ['accuracy', 'loss']\n",
      "...Training: start of batch 25; got log keys: []\n",
      "...Training: end of batch 25; got log keys: ['accuracy', 'loss']\n",
      "Start testing; got log keys: []\n",
      "...Evaluating: start of batch 0; got log keys: []\n",
      "...Evaluating: end of batch 0; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 1; got log keys: []\n",
      "...Evaluating: end of batch 1; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 2; got log keys: []\n",
      "...Evaluating: end of batch 2; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 3; got log keys: []\n",
      "...Evaluating: end of batch 3; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 4; got log keys: []\n",
      "...Evaluating: end of batch 4; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 5; got log keys: []\n",
      "...Evaluating: end of batch 5; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 6; got log keys: []\n",
      "...Evaluating: end of batch 6; got log keys: ['accuracy', 'loss']\n",
      "Stop testing; got log keys: ['accuracy', 'loss']\n",
      "End epoch 1 of training; got log keys: ['accuracy', 'loss', 'val_accuracy', 'val_loss']\n",
      "stoping training : got log keys: ['accuracy', 'loss', 'val_accuracy', 'val_loss']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1564d695540>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(dense_units=256)\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "model.fit(train_batches, \n",
    "          epochs=2, \n",
    "          validation_data=validation_batches, \n",
    "          verbose=0,\n",
    "          callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fc5fdb4d-bde9-4435-b5b8-66066e79a8bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing; got log keys: []\n",
      "...Evaluating: start of batch 0; got log keys: []\n",
      "...Evaluating: end of batch 0; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 1; got log keys: []\n",
      "...Evaluating: end of batch 1; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 2; got log keys: []\n",
      "...Evaluating: end of batch 2; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 3; got log keys: []\n",
      "...Evaluating: end of batch 3; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 4; got log keys: []\n",
      "...Evaluating: end of batch 4; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 5; got log keys: []\n",
      "...Evaluating: end of batch 5; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 6; got log keys: []\n",
      "...Evaluating: end of batch 6; got log keys: ['accuracy', 'loss']\n",
      "...Evaluating: start of batch 7; got log keys: []\n",
      "...Evaluating: end of batch 7; got log keys: ['accuracy', 'loss']\n",
      "Stop testing; got log keys: ['accuracy', 'loss']\n",
      "Start predicting; got log keys: []\n",
      "...Predicting: start of batch 0; got log keys: []\n",
      "...Predicting: end of batch 0; got log keys: ['outputs']\n",
      "...Predicting: start of batch 1; got log keys: []m \u001b[1m0s\u001b[0m 59ms/step\n",
      "...Predicting: end of batch 1; got log keys: ['outputs']\n",
      "...Predicting: start of batch 2; got log keys: []\n",
      "...Predicting: end of batch 2; got log keys: ['outputs']\n",
      "...Predicting: start of batch 3; got log keys: []\n",
      "...Predicting: end of batch 3; got log keys: ['outputs']\n",
      "...Predicting: start of batch 4; got log keys: []m \u001b[1m0s\u001b[0m 17ms/step\n",
      "...Predicting: end of batch 4; got log keys: ['outputs']\n",
      "...Predicting: start of batch 5; got log keys: []\n",
      "...Predicting: end of batch 5; got log keys: ['outputs']\n",
      "...Predicting: start of batch 6; got log keys: []\n",
      "...Predicting: end of batch 6; got log keys: ['outputs']\n",
      "...Predicting: start of batch 7; got log keys: []\n",
      "...Predicting: end of batch 7; got log keys: ['outputs']\n",
      "Stop predicting; got log keys: []━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(\n",
    "    test_batches, verbose=0, callbacks=[CustomCallback()]\n",
    ")\n",
    "\n",
    "res = model.predict(test_batches, batch_size=128, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e7ab6-b1c0-4590-8ee9-9efc0e84d2b1",
   "metadata": {
    "id": "vSKt8minQliZ"
   },
   "source": [
    "### Usage of `logs` dict\n",
    "The `logs` dict contains the loss value, and all the metrics at the end of a batch or\n",
    "epoch. Example includes the loss and mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "61f5604c-7c28-454b-86a5-0a4181f6fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndErrorPrintingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(f'Up to batch {batch}, the average loss is {logs[\"loss\"]:7.2f}')\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(f'Up to batch {batch}, the average loss is {logs[\"loss\"]:.2f}')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'The average loss for epoch {epoch} is {logs[\"loss\"]:7.2f} '\n",
    "              f'and accuracy is {logs[\"accuracy\"]:7.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a8c93-a75e-405d-b0db-f01c5dcb6a69",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Let’s examine this topic in detail:\n",
    "\n",
    "### Main Line of Code:\n",
    "```python\n",
    "\"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "```\n",
    "Here, `:7.2f` is a **format specifier** in Python’s `.format()` method that controls how `logs[\"loss\"]` is displayed. Let’s break it down:\n",
    "\n",
    "- **`:7`**: The number 7 sets the minimum field width. The value will occupy at least 7 characters. If the number is shorter, it is padded with spaces on the left.\n",
    "- **`.2f`**: This part means display the number as a floating point value with exactly 2 decimal places.\n",
    "- **Combination `:7.2f`**: So, the number is shown as a float with 2 decimal digits, and the whole string will be at least 7 characters long (including the decimal point and digits). For example:\n",
    "    - If `logs[\"loss\"] = 12.345`, the output for `:7.2f` will be: `  12.34` (2 spaces + \"12.34\" = 7 characters).\n",
    "    - If `logs[\"loss\"] = 5.6`, it will be: `   5.60` (3 spaces + \"5.60\" = 7 characters).\n",
    "\n",
    "***\n",
    "\n",
    "### Provided Line of Code (f-string):\n",
    "In the version I suggested:\n",
    "```python\n",
    "print(f'Up to batch {batch}, the average loss is {logs[\"loss\"]:.2f}')\n",
    "```\n",
    "- **`:2f`**: In this f-string, this only specifies 2 digits after the decimal point but does not set the minimum field width.\n",
    "- **Result**: The field width is dynamic (just as long as the number needs). For example:\n",
    "    - If `logs[\"loss\"] = 12.345`, output is `12.34` (no extra spaces).\n",
    "    - If `logs[\"loss\"] = 5.6`, output is `5.60`.\n",
    "\n",
    "***\n",
    "\n",
    "### Difference & How to Match:\n",
    "The `:7` specifier in the original code ensures a fixed minimum width of 7 characters, but the initial f-string version did not include this. To make the f-string behave identically, set the field width with `:7.2f`.\n",
    "\n",
    "***\n",
    "\n",
    "### Fully Equivalent f-string Version:\n",
    "```python\n",
    "class LossAndErrorPrintingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(f'Up to batch {batch}, the average loss is {logs[\"loss\"]:7.2f}')\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(f'Up to batch {batch}, the average loss is {logs[\"loss\"]:7.2f}')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'The average loss for epoch {epoch} is {logs[\"loss\"]:7.2f} '\n",
    "              f'and accuracy is {logs[\"accuracy\"]:7.2f}')\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Additional Notes:\n",
    "- **Adding `:7.2f` in the f-string**: Now, the field width is 7 characters, exactly like the original version. If the number is shorter, it will be padded with spaces from the left.\n",
    "- **Example outputs**:\n",
    "    - If `logs[\"loss\"] = 12.345`, the output is: `  12.34` (exactly like the original).\n",
    "    - If `logs[\"accuracy\"] = 0.98`, the output is: `   0.98`.\n",
    "\n",
    "***\n",
    "\n",
    "### Summary:\n",
    "- `:7` sets the minimum field width to 7 characters; numbers shorter than that are left-padded with spaces.\n",
    "- The initial f-string version missed this, but using `{logs[\"loss\"]:7.2f}` (in the f-string) makes it fully equivalent to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c23578f9-850b-4a02-a7b6-015636ce904c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to batch 0, the average loss is    0.22\n",
      "Up to batch 1, the average loss is    0.24\n",
      "Up to batch 2, the average loss is    0.24\n",
      "Up to batch 3, the average loss is    0.24\n",
      "Up to batch 4, the average loss is    0.24\n",
      "Up to batch 5, the average loss is    0.25\n",
      "Up to batch 6, the average loss is    0.24\n",
      "Up to batch 7, the average loss is    0.25\n",
      "Up to batch 8, the average loss is    0.24\n",
      "Up to batch 9, the average loss is    0.24\n",
      "Up to batch 10, the average loss is    0.24\n",
      "Up to batch 11, the average loss is    0.24\n",
      "Up to batch 12, the average loss is    0.24\n",
      "Up to batch 13, the average loss is    0.24\n",
      "Up to batch 14, the average loss is    0.24\n",
      "Up to batch 15, the average loss is    0.24\n",
      "Up to batch 16, the average loss is    0.23\n",
      "Up to batch 17, the average loss is    0.23\n",
      "Up to batch 18, the average loss is    0.23\n",
      "Up to batch 19, the average loss is    0.23\n",
      "Up to batch 20, the average loss is    0.23\n",
      "Up to batch 21, the average loss is    0.22\n",
      "Up to batch 22, the average loss is    0.22\n",
      "Up to batch 23, the average loss is    0.23\n",
      "Up to batch 24, the average loss is    0.23\n",
      "Up to batch 25, the average loss is    0.23\n",
      "The average loss for epoch 0 is    0.23 and accuracy is    0.93\n",
      "Up to batch 0, the average loss is    0.15\n",
      "Up to batch 1, the average loss is    0.17\n",
      "Up to batch 2, the average loss is    0.19\n",
      "Up to batch 3, the average loss is    0.20\n",
      "Up to batch 4, the average loss is    0.19\n",
      "Up to batch 5, the average loss is    0.18\n",
      "Up to batch 6, the average loss is    0.19\n",
      "Up to batch 7, the average loss is    0.18\n",
      "Up to batch 8, the average loss is    0.19\n",
      "Up to batch 9, the average loss is    0.20\n",
      "Up to batch 10, the average loss is    0.20\n",
      "Up to batch 11, the average loss is    0.20\n",
      "Up to batch 12, the average loss is    0.20\n",
      "Up to batch 13, the average loss is    0.20\n",
      "Up to batch 14, the average loss is    0.20\n",
      "Up to batch 15, the average loss is    0.20\n",
      "Up to batch 16, the average loss is    0.20\n",
      "Up to batch 17, the average loss is    0.19\n",
      "Up to batch 18, the average loss is    0.19\n",
      "Up to batch 19, the average loss is    0.19\n",
      "Up to batch 20, the average loss is    0.19\n",
      "Up to batch 21, the average loss is    0.19\n",
      "Up to batch 22, the average loss is    0.19\n",
      "Up to batch 23, the average loss is    0.19\n",
      "Up to batch 24, the average loss is    0.19\n",
      "Up to batch 25, the average loss is    0.19\n",
      "The average loss for epoch 1 is    0.19 and accuracy is    0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1564ea54730>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_batches,\n",
    "    epochs=2,\n",
    "    verbose=0,\n",
    "    callbacks=[LossAndErrorPrintingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d8465287-bae5-4757-a1ea-6ee9787f593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to batch 0, the average loss is 0.88\n",
      "Up to batch 1, the average loss is 0.87\n",
      "Up to batch 2, the average loss is 1.06\n",
      "Up to batch 3, the average loss is 0.99\n",
      "Up to batch 4, the average loss is 1.02\n",
      "Up to batch 5, the average loss is 0.98\n",
      "Up to batch 6, the average loss is 1.11\n",
      "Up to batch 7, the average loss is 1.11\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(\n",
    "    test_batches,\n",
    "    verbose=0,\n",
    "    callbacks=[LossAndErrorPrintingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0400c9-0f9a-489f-979f-c8d213189fa1",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Using the `self.model` Attribute\n",
    "\n",
    "In addition to receiving log information during the invocation of a Callback method, these Callbacks also have access to the model associated with the current training, evaluation, or inference round via `self.model`.\n",
    "\n",
    "Here are some things you can do with `self.model` in a Callback:\n",
    "\n",
    "- Set `self.model.stop_training = True` to immediately stop the training process.\n",
    "- Change optimizer parameters (such as `self.model.optimizer.learning_rate`) through `self.model.optimizer`.\n",
    "- Save the model at certain intervals.\n",
    "- Log the output of `model.predict()` on some test samples at the end of each epoch to monitor accuracy during training.\n",
    "- Extract and visualize intermediate feature representations at the end of each epoch to observe what the model is learning over time.\n",
    "- Etc.\n",
    "\n",
    "Let’s look at these concepts in practice with a few examples.\n",
    "\n",
    "***\n",
    "\n",
    "### A Callback for Detecting Overfitting\n",
    "\n",
    "Let’s review a Callback that measures the ratio between the validation loss and the training loss. If this ratio gets too high, it might signal overfitting—since the validation loss is no longer decreasing while the training loss keeps dropping, causing the ratio to rise. In this case, training should be stopped to prevent further overfitting.\n",
    "\n",
    "#### Practical Example:\n",
    "\n",
    "To implement this idea, we can write a custom Callback that calculates this ratio and stops training if it exceeds a certain threshold:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "class OverfittingDetectorCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold=1.5):\n",
    "        super(OverfittingDetectorCallback, self).__init__()\n",
    "        self.threshold = threshold  # The loss ratio threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_loss = logs.get('val_loss')\n",
    "        train_loss = logs.get('loss')\n",
    "\n",
    "        if val_loss is not None and train_loss is not None:\n",
    "            ratio = val_loss / train_loss\n",
    "            print(f'Ratio of val_loss to train_loss at epoch {epoch}: {ratio:.2f}')\n",
    "            if ratio > self.threshold:\n",
    "                print(f'Overfitting detected! Ratio {ratio:.2f} exceeds threshold {self.threshold}. Stopping training...')\n",
    "                self.model.stop_training = True\n",
    "\n",
    "# Using the callback in your model\n",
    "model = tf.keras.Sequential([...])  # Define your model layers\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50,\n",
    "          callbacks=[OverfittingDetectorCallback(threshold=1.5)])\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "- **Calculating the Ratio**: At the end of each epoch, the ratio of validation loss to training loss is computed.\n",
    "- **Overfitting Detection**: If this ratio exceeds the threshold (e.g., 1.5), overfitting is assumed and training is stopped.\n",
    "- **Access to `self.model`**: The training loop is halted by setting `self.model.stop_training = True`.\n",
    "- **Flexibility**: The threshold is adjustable and can be fine-tuned for the project’s needs.\n",
    "\n",
    "This Callback helps prevent wasting computation resources by stopping training when overfitting is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "995f4ee1-13bf-4043-851b-800ff82159df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectOverfittingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,threshold=0.7):\n",
    "        super(DetectOverfittingCallback,self).__init__()\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        ratio = logs['val_loss']/logs['loss']\n",
    "        print(f\"Epoch: {epoch}, Val/Train loss ratio: {ratio:.2f}\")\n",
    "\n",
    "        if ratio >self.threshold:\n",
    "            print(\"Stopping training...\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd27d4-dcdb-4560-afb5-8e7f2797a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(dense_units=256)\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "model.fit(train_batches, \n",
    "          epochs=10, \n",
    "          validation_data=validation_batches, \n",
    "          verbose=0,\n",
    "          callbacks=[DetectOverfittingCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826e78d-9927-43dc-85de-00c65cc6cae1",
   "metadata": {},
   "source": [
    "# Visualizing at the end of each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24eb656-61c7-4669-9cc0-3be30474028a",
   "metadata": {},
   "source": [
    "We do this to make sure that our accuracy is not high only for a single class, and that a high accuracy score is not just because of one class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42a068-f726-4e76-88b7-8e5feaf1118c",
   "metadata": {},
   "source": [
    "In fact, our model should not remain unprocessed for a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766c185-6086-4dd7-8287-809f9618b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "# گام 1: آماده‌سازی داده‌ها\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # نرمال‌سازی داده‌ها\n",
    "\n",
    "# انتخاب 5 نمونه برای ویژوال‌سازی\n",
    "num_samples = 5\n",
    "test_samples = x_test[:num_samples]\n",
    "true_labels = y_test[:num_samples]\n",
    "\n",
    "# گام 2: تعریف مدل\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2143e-48a4-4182-803f-96140b93fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# گام 3: کامپایل مدل\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d8b7b-c036-4f1c-b98c-b37f8e545044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# گام 4: تعریف Callback سفارشی\n",
    "class VisualizePredictionsCallback(Callback):\n",
    "    def __init__(self, test_samples, true_labels, num_samples):\n",
    "        super(VisualizePredictionsCallback, self).__init__()\n",
    "        self.test_samples = test_samples\n",
    "        self.true_labels = true_labels\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # گام 5: پیش‌بینی مدل\n",
    "        predictions = self.model.predict(self.test_samples, verbose=0)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # گام 6: ویژوال‌سازی\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        for i in range(self.num_samples):\n",
    "            plt.subplot(1, num_samples, i + 1)\n",
    "            plt.imshow(test_samples[i], cmap='gray')\n",
    "            plt.title(f'True: {true_labels[i]}\\nPred: {predicted_labels[i]}')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f'Epoch {epoch + 1} Predictions')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a86bd2-e20d-4d0c-8c28-6dad42e8aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# گام 7: آموزش مدل با Callback\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[VisualizePredictionsCallback(test_samples, true_labels, num_samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66da36b-d000-4c44-aafc-eb59ddb26a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
